# ğŸš€ Gemma LoRA Fine-Tuner - Production No-Code Platform

**A production-ready, no-code web application for fine-tuning Google Gemma models using LoRA, powered by Unsloth for fast and memory-efficient training.**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-green.svg)](https://fastapi.tiangolo.com/)
[![Gradio](https://img.shields.io/badge/Gradio-4.0+-orange.svg)](https://gradio.app/)
[![Docker](https://img.shields.io/badge/Docker-GPU-blue.svg)](https://www.docker.com/)

## ğŸŒŸ Features

- **ğŸ¨ No-Code Interface** - User-friendly Gradio UI for non-technical users
- **ğŸ“Š Multi-Format Dataset Support** - Upload CSV, JSON, TXT files
- **ğŸ” Auto Dataset Validation** - Intelligent preprocessing and conversion
- **âš¡ Unsloth-Powered Training** - 2x faster training with 60% less memory
- **ğŸ¯ LoRA Fine-Tuning** - Efficient parameter-efficient fine-tuning
- **ğŸ“ˆ Real-Time Progress** - Live training metrics and progress tracking
- **ğŸ’¾ Model Export** - Download LoRA adapters or merged models
- **ğŸ” Secure Backend** - FastAPI with production-grade security
- **ğŸ³ Docker + GPU Ready** - Containerized deployment with NVIDIA GPU support
- **ğŸ“¦ Single GPU Optimized** - Runs on consumer-grade GPUs (RTX 3060+)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Gradio Frontend (UI)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Dataset    â”‚ â”‚   Training   â”‚ â”‚    Model     â”‚        â”‚
â”‚  â”‚   Upload     â”‚ â”‚   Progress   â”‚ â”‚    Export    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†• HTTP/WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI Backend (API)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Upload     â”‚ â”‚   Training   â”‚ â”‚    Export    â”‚        â”‚
â”‚  â”‚   Endpoint   â”‚ â”‚   Manager    â”‚ â”‚   Endpoint   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Unsloth Training Engine (Core)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Gemma      â”‚ â”‚     LoRA     â”‚ â”‚   Progress   â”‚        â”‚
â”‚  â”‚   Loader     â”‚ â”‚   Training   â”‚ â”‚   Tracker    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
gemma-finetuner/
â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                # FastAPI application entry point
â”‚   â”œâ”€â”€ config.py              # Configuration and settings
â”‚   â”œâ”€â”€ models.py              # Pydantic models
â”‚   â”œâ”€â”€ training/              # Training logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py         # Unsloth training core
â”‚   â”‚   â”œâ”€â”€ progress.py        # Progress tracking
â”‚   â”‚   â””â”€â”€ callbacks.py       # Training callbacks
â”‚   â”œâ”€â”€ preprocessing/         # Dataset preprocessing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py          # Dataset loaders
â”‚   â”‚   â””â”€â”€ validator.py       # Validation logic
â”‚   â””â”€â”€ utils/                 # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ gpu_utils.py       # GPU memory management
â”œâ”€â”€ frontend/                  # Gradio frontend
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                 # Gradio UI
â”‚   â””â”€â”€ components/            # UI components
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ upload.py          # Upload interface
â”‚       â”œâ”€â”€ training.py        # Training interface
â”‚       â””â”€â”€ export.py          # Export interface
â”œâ”€â”€ datasets/                  # Uploaded datasets storage
â”œâ”€â”€ models/                    # Model cache
â”œâ”€â”€ exports/                   # Exported models
â”œâ”€â”€ logs/                      # Training logs
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ Dockerfile                 # Docker configuration
â”œâ”€â”€ docker-compose.yml         # Docker Compose setup
â”œâ”€â”€ .env.example              # Environment variables template
â””â”€â”€ README.md                  # This file
```

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **CUDA 11.8+** (for GPU support)
- **16GB+ RAM**
- **NVIDIA GPU** with 8GB+ VRAM (RTX 3060 12GB or better recommended)
- **Docker** (optional, for containerized deployment)

### 1. Local Installation

```bash
# Clone the repository
git clone <repository-url>
cd gemma-finetuner

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env with your settings
```

### 2. Run the Application

#### Option A: Run Backend and Frontend Separately

**Terminal 1 - Backend:**
```bash
cd backend
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

**Terminal 2 - Frontend:**
```bash
cd frontend
python app.py
```

#### Option B: Run with Docker (Recommended)

```bash
# Build and run with GPU support
docker-compose up --build

# Access the application
# Frontend: http://localhost:7860
# Backend API: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

## ğŸ“– User Guide

### 1. Upload Dataset

1. Navigate to the **Upload** tab
2. Select your dataset file (CSV, JSON, or TXT)
3. Configure dataset format:
   - **CSV**: Specify text column and optional label column
   - **JSON**: Specify keys for text and labels
   - **TXT**: One sample per line
4. Click **Upload & Validate**

### 2. Configure Training

1. Go to the **Training** tab
2. Select uploaded dataset
3. Configure parameters:
   - **Model**: Choose Gemma variant (gemma-2b, gemma-7b)
   - **LoRA Rank**: 8, 16, 32 (higher = more parameters)
   - **LoRA Alpha**: Usually 16 or 32
   - **Epochs**: Number of training iterations
   - **Batch Size**: Adjust based on GPU memory
   - **Learning Rate**: Usually 2e-4 to 3e-4
4. Click **Start Training**

### 3. Monitor Progress

- Real-time progress bar
- Live loss metrics
- GPU memory usage
- Estimated time remaining
- Training logs

### 4. Export Model

1. Navigate to **Export** tab
2. Select completed training run
3. Choose export format:
   - **LoRA Adapters Only** (small, ~100MB)
   - **Merged Model** (full model, ~5GB+)
4. Click **Export & Download**

## âš™ï¸ Configuration

### Environment Variables (.env)

```env
# API Settings
API_HOST=0.0.0.0
API_PORT=8000
GRADIO_PORT=7860

# Storage Paths
DATASETS_DIR=./datasets
MODELS_DIR=./models
EXPORTS_DIR=./exports
LOGS_DIR=./logs

# Model Settings
DEFAULT_MODEL=unsloth/gemma-2b-bnb-4bit
MAX_SEQ_LENGTH=2048
LOAD_IN_4BIT=true

# Training Defaults
DEFAULT_LORA_R=16
DEFAULT_LORA_ALPHA=16
DEFAULT_EPOCHS=3
DEFAULT_BATCH_SIZE=2
DEFAULT_LEARNING_RATE=2e-4

# GPU Settings
CUDA_VISIBLE_DEVICES=0
MAX_MEMORY_GB=12
```

## ğŸ³ Docker Deployment

### Docker Compose (Recommended)

```bash
# Start services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Manual Docker Build

```bash
# Build image
docker build -t gemma-finetuner:latest .

# Run with GPU
docker run --gpus all \
  -p 8000:8000 \
  -p 7860:7860 \
  -v $(pwd)/datasets:/app/datasets \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/exports:/app/exports \
  gemma-finetuner:latest
```

## ğŸ“Š Supported Models

- **Gemma 2B** - `unsloth/gemma-2b-bnb-4bit` (Recommended for 8GB VRAM)
- **Gemma 7B** - `unsloth/gemma-7b-bnb-4bit` (Requires 12GB+ VRAM)
- **Gemma 2B Instruct** - `unsloth/gemma-2b-it-bnb-4bit`
- **Gemma 7B Instruct** - `unsloth/gemma-7b-it-bnb-4bit`

## ğŸ¯ Dataset Format Examples

### CSV Format

```csv
text,label
"Sample text for training",category_a
"Another training example",category_b
```

### JSON Format

```json
[
  {"text": "Sample text for training", "label": "category_a"},
  {"text": "Another training example", "label": "category_b"}
]
```

### TXT Format

```
Sample text for training
Another training example
```

## ğŸ”§ Troubleshooting

### Out of Memory (OOM) Errors

1. Reduce `batch_size` to 1
2. Reduce `max_seq_length` to 1024
3. Use smaller model (gemma-2b instead of gemma-7b)
4. Enable gradient checkpointing (already enabled)

### Slow Training

1. Increase `batch_size` if memory allows
2. Use mixed precision (FP16/BF16) - already enabled
3. Ensure CUDA is properly installed
4. Check GPU utilization with `nvidia-smi`

### Dataset Upload Fails

1. Check file format matches specification
2. Ensure file size < 500MB
3. Verify CSV has correct headers
4. Check JSON is valid format

## ğŸ“š API Documentation

### Interactive API Docs

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Key Endpoints

- `POST /api/upload` - Upload dataset
- `POST /api/train` - Start training
- `GET /api/progress/{job_id}` - Get training progress
- `GET /api/export/{job_id}` - Export fine-tuned model
- `GET /api/jobs` - List all training jobs

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- **Unsloth AI** - For the amazing fast training library
- **Google** - For the Gemma models
- **Hugging Face** - For Transformers and PEFT
- **Gradio** - For the excellent UI framework

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/gemma-finetuner/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/gemma-finetuner/discussions)

---

**Built with â¤ï¸ for the ML Community**
#   G E M M A - N O - C O D E - G e m m a - L o R A - F i n e - T u n e r 
 
 